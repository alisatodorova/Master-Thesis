Using DataParallel on GPUs: [0, 1]
GPU 0: NVIDIA GeForce RTX 4060 Ti with 8187.5 MB memory
GPU 1: NVIDIA GeForce RTX 4060 Ti with 8187.38 MB memory
Using device: cuda
DATASET SUMMARY
Number of classes        : 47
Class labels             : ['addisplay', 'addisplay++adware', 'adload', 'adsware', 'adware', 'adware++adware', 'adware++grayware++virus', 'adware++virus', 'adwareare', 'backdoor', 'banker++trojan', 'benign', 'click', 'clicker', 'clicker++trojan', 'clickfraud++riskware', 'downloader', 'dropper++trojan', 'exploit', 'fakeangry', 'fakeapp', 'fakeapp++trojan', 'fakeinst++trojan', 'gray', 'hacktool', 'malware', 'malware++trj', 'monitor', 'ransom++trojan', 'risktool++riskware++virus', 'riskware', 'riskware++smssend', 'rog', 'rootnik++trojan', 'smssend', 'smssend++trojan', 'spr', 'spy', 'spy++trojan', 'spyware', 'trj', 'troj', 'trojan', 'trojandownloader', 'trojandropper', 'virus', 'worm']
Train samples            : 883416
Validation samples       : 126202
Test samples             : 252406
Train class distribution : {0: 12221, 1: 206, 2: 233, 3: 1856, 4: 619118, 5: 1752, 6: 585, 7: 192, 8: 106, 9: 421, 10: 774, 11: 55383, 12: 79, 13: 186, 14: 2007, 15: 258, 16: 3498, 17: 414, 18: 3907, 19: 148, 20: 298, 21: 179, 22: 503, 23: 645, 24: 379, 25: 1754, 26: 426, 27: 950, 28: 807, 29: 106, 30: 22236, 31: 173, 32: 1382, 33: 156, 34: 2286, 35: 3006, 36: 9675, 37: 1152, 38: 83, 39: 4613, 40: 658, 41: 2316, 42: 125478, 43: 398, 44: 125, 45: 134, 46: 154}
--------------------------------------------------
Using 2 GPUs.
MODEL SUMMARY
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 128, 128]           9,408
            Conv2d-2         [-1, 64, 128, 128]           9,408
       BatchNorm2d-3         [-1, 64, 128, 128]             128
       BatchNorm2d-4         [-1, 64, 128, 128]             128
              ReLU-5         [-1, 64, 128, 128]               0
              ReLU-6         [-1, 64, 128, 128]               0
         MaxPool2d-7           [-1, 64, 64, 64]               0
         MaxPool2d-8           [-1, 64, 64, 64]               0
            Conv2d-9           [-1, 64, 64, 64]          36,864
           Conv2d-10           [-1, 64, 64, 64]          36,864
      BatchNorm2d-11           [-1, 64, 64, 64]             128
      BatchNorm2d-12           [-1, 64, 64, 64]             128
             ReLU-13           [-1, 64, 64, 64]               0
             ReLU-14           [-1, 64, 64, 64]               0
           Conv2d-15           [-1, 64, 64, 64]          36,864
           Conv2d-16           [-1, 64, 64, 64]          36,864
      BatchNorm2d-17           [-1, 64, 64, 64]             128
      BatchNorm2d-18           [-1, 64, 64, 64]             128
             ReLU-19           [-1, 64, 64, 64]               0
       BasicBlock-20           [-1, 64, 64, 64]               0
             ReLU-21           [-1, 64, 64, 64]               0
       BasicBlock-22           [-1, 64, 64, 64]               0
           Conv2d-23           [-1, 64, 64, 64]          36,864
           Conv2d-24           [-1, 64, 64, 64]          36,864
      BatchNorm2d-25           [-1, 64, 64, 64]             128
      BatchNorm2d-26           [-1, 64, 64, 64]             128
             ReLU-27           [-1, 64, 64, 64]               0
             ReLU-28           [-1, 64, 64, 64]               0
           Conv2d-29           [-1, 64, 64, 64]          36,864
      BatchNorm2d-30           [-1, 64, 64, 64]             128
           Conv2d-31           [-1, 64, 64, 64]          36,864
             ReLU-32           [-1, 64, 64, 64]               0
       BasicBlock-33           [-1, 64, 64, 64]               0
      BatchNorm2d-34           [-1, 64, 64, 64]             128
             ReLU-35           [-1, 64, 64, 64]               0
       BasicBlock-36           [-1, 64, 64, 64]               0
           Conv2d-37          [-1, 128, 32, 32]          73,728
           Conv2d-38          [-1, 128, 32, 32]          73,728
      BatchNorm2d-39          [-1, 128, 32, 32]             256
      BatchNorm2d-40          [-1, 128, 32, 32]             256
             ReLU-41          [-1, 128, 32, 32]               0
             ReLU-42          [-1, 128, 32, 32]               0
           Conv2d-43          [-1, 128, 32, 32]         147,456
           Conv2d-44          [-1, 128, 32, 32]         147,456
      BatchNorm2d-45          [-1, 128, 32, 32]             256
      BatchNorm2d-46          [-1, 128, 32, 32]             256
           Conv2d-47          [-1, 128, 32, 32]           8,192
           Conv2d-48          [-1, 128, 32, 32]           8,192
      BatchNorm2d-49          [-1, 128, 32, 32]             256
      BatchNorm2d-50          [-1, 128, 32, 32]             256
             ReLU-51          [-1, 128, 32, 32]               0
             ReLU-52          [-1, 128, 32, 32]               0
       BasicBlock-53          [-1, 128, 32, 32]               0
       BasicBlock-54          [-1, 128, 32, 32]               0
           Conv2d-55          [-1, 128, 32, 32]         147,456
           Conv2d-56          [-1, 128, 32, 32]         147,456
      BatchNorm2d-57          [-1, 128, 32, 32]             256
             ReLU-58          [-1, 128, 32, 32]               0
      BatchNorm2d-59          [-1, 128, 32, 32]             256
             ReLU-60          [-1, 128, 32, 32]               0
           Conv2d-61          [-1, 128, 32, 32]         147,456
           Conv2d-62          [-1, 128, 32, 32]         147,456
      BatchNorm2d-63          [-1, 128, 32, 32]             256
      BatchNorm2d-64          [-1, 128, 32, 32]             256
             ReLU-65          [-1, 128, 32, 32]               0
       BasicBlock-66          [-1, 128, 32, 32]               0
             ReLU-67          [-1, 128, 32, 32]               0
       BasicBlock-68          [-1, 128, 32, 32]               0
           Conv2d-69          [-1, 256, 16, 16]         294,912
           Conv2d-70          [-1, 256, 16, 16]         294,912
      BatchNorm2d-71          [-1, 256, 16, 16]             512
      BatchNorm2d-72          [-1, 256, 16, 16]             512
             ReLU-73          [-1, 256, 16, 16]               0
             ReLU-74          [-1, 256, 16, 16]               0
           Conv2d-75          [-1, 256, 16, 16]         589,824
           Conv2d-76          [-1, 256, 16, 16]         589,824
      BatchNorm2d-77          [-1, 256, 16, 16]             512
      BatchNorm2d-78          [-1, 256, 16, 16]             512
           Conv2d-79          [-1, 256, 16, 16]          32,768
           Conv2d-80          [-1, 256, 16, 16]          32,768
      BatchNorm2d-81          [-1, 256, 16, 16]             512
      BatchNorm2d-82          [-1, 256, 16, 16]             512
             ReLU-83          [-1, 256, 16, 16]               0
       BasicBlock-84          [-1, 256, 16, 16]               0
             ReLU-85          [-1, 256, 16, 16]               0
       BasicBlock-86          [-1, 256, 16, 16]               0
           Conv2d-87          [-1, 256, 16, 16]         589,824
           Conv2d-88          [-1, 256, 16, 16]         589,824
      BatchNorm2d-89          [-1, 256, 16, 16]             512
      BatchNorm2d-90          [-1, 256, 16, 16]             512
             ReLU-91          [-1, 256, 16, 16]               0
           Conv2d-92          [-1, 256, 16, 16]         589,824
             ReLU-93          [-1, 256, 16, 16]               0
      BatchNorm2d-94          [-1, 256, 16, 16]             512
           Conv2d-95          [-1, 256, 16, 16]         589,824
             ReLU-96          [-1, 256, 16, 16]               0
       BasicBlock-97          [-1, 256, 16, 16]               0
      BatchNorm2d-98          [-1, 256, 16, 16]             512
             ReLU-99          [-1, 256, 16, 16]               0
      BasicBlock-100          [-1, 256, 16, 16]               0
          Conv2d-101            [-1, 512, 8, 8]       1,179,648
     BatchNorm2d-102            [-1, 512, 8, 8]           1,024
          Conv2d-103            [-1, 512, 8, 8]       1,179,648
            ReLU-104            [-1, 512, 8, 8]               0
     BatchNorm2d-105            [-1, 512, 8, 8]           1,024
            ReLU-106            [-1, 512, 8, 8]               0
          Conv2d-107            [-1, 512, 8, 8]       2,359,296
          Conv2d-108            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-109            [-1, 512, 8, 8]           1,024
     BatchNorm2d-110            [-1, 512, 8, 8]           1,024
          Conv2d-111            [-1, 512, 8, 8]         131,072
          Conv2d-112            [-1, 512, 8, 8]         131,072
     BatchNorm2d-113            [-1, 512, 8, 8]           1,024
            ReLU-114            [-1, 512, 8, 8]               0
      BasicBlock-115            [-1, 512, 8, 8]               0
     BatchNorm2d-116            [-1, 512, 8, 8]           1,024
          Conv2d-117            [-1, 512, 8, 8]       2,359,296
            ReLU-118            [-1, 512, 8, 8]               0
      BasicBlock-119            [-1, 512, 8, 8]               0
     BatchNorm2d-120            [-1, 512, 8, 8]           1,024
          Conv2d-121            [-1, 512, 8, 8]       2,359,296
            ReLU-122            [-1, 512, 8, 8]               0
          Conv2d-123            [-1, 512, 8, 8]       2,359,296
     BatchNorm2d-124            [-1, 512, 8, 8]           1,024
            ReLU-125            [-1, 512, 8, 8]               0
     BatchNorm2d-126            [-1, 512, 8, 8]           1,024
          Conv2d-127            [-1, 512, 8, 8]       2,359,296
            ReLU-128            [-1, 512, 8, 8]               0
      BasicBlock-129            [-1, 512, 8, 8]               0
     BatchNorm2d-130            [-1, 512, 8, 8]           1,024
            ReLU-131            [-1, 512, 8, 8]               0
      BasicBlock-132            [-1, 512, 8, 8]               0
AdaptiveAvgPool2d-133            [-1, 512, 1, 1]               0
AdaptiveAvgPool2d-134            [-1, 512, 1, 1]               0
          Linear-135                   [-1, 47]          24,111
          ResNet-136                   [-1, 47]               0
          Linear-137                   [-1, 47]          24,111
          ResNet-138                   [-1, 47]               0
================================================================
Total params: 22,401,246
Trainable params: 22,401,246
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.75
Forward/backward pass size (MB): 164.01
Params size (MB): 85.45
Estimated Total Size (MB): 250.21
----------------------------------------------------------------
Loaded epoch 80

Epoch 081 | Time: 4022.1s | Loss: 1848.1673
Train Acc: 0.9497 | Train Balanced Acc: 0.8988
Val   Acc: 0.8271 | Val   Balanced Acc: 0.5014

Epoch 082 | Time: 3929.5s | Loss: 1821.6891
Train Acc: 0.9500 | Train Balanced Acc: 0.9032
Val   Acc: 0.8217 | Val   Balanced Acc: 0.5042

Epoch 083 | Time: 3906.0s | Loss: 1805.7761
Train Acc: 0.9506 | Train Balanced Acc: 0.9014
Val   Acc: 0.8240 | Val   Balanced Acc: 0.5040

Epoch 084 | Time: 3922.1s | Loss: 1771.9084
Train Acc: 0.9517 | Train Balanced Acc: 0.9022
Val   Acc: 0.8245 | Val   Balanced Acc: 0.5051

Epoch 085 | Time: 3916.2s | Loss: 1759.4528
Train Acc: 0.9520 | Train Balanced Acc: 0.9038
Val   Acc: 0.8248 | Val   Balanced Acc: 0.5146

Epoch 086 | Time: 3919.3s | Loss: 1753.3975
Train Acc: 0.9521 | Train Balanced Acc: 0.9044
Val   Acc: 0.8259 | Val   Balanced Acc: 0.5091

Epoch 087 | Time: 3998.4s | Loss: 1736.4951
Train Acc: 0.9524 | Train Balanced Acc: 0.9072
Val   Acc: 0.8235 | Val   Balanced Acc: 0.5164

Epoch 088 | Time: 3901.3s | Loss: 1714.7605
Train Acc: 0.9530 | Train Balanced Acc: 0.9079
Val   Acc: 0.8250 | Val   Balanced Acc: 0.4996

Epoch 089 | Time: 4000.8s | Loss: 1705.7701
Train Acc: 0.9532 | Train Balanced Acc: 0.9060
Val   Acc: 0.8243 | Val   Balanced Acc: 0.4989

Epoch 090 | Time: 3903.1s | Loss: 1693.6313
Train Acc: 0.9535 | Train Balanced Acc: 0.9065
Val   Acc: 0.8227 | Val   Balanced Acc: 0.5028
Checkpoint saved at: C:\Users\Boss\Downloads\malnet-images\checkpoints\epoch_090.pt

Epoch 091 | Time: 3923.9s | Loss: 1669.4171
Train Acc: 0.9540 | Train Balanced Acc: 0.9106
Val   Acc: 0.8243 | Val   Balanced Acc: 0.5113

Epoch 092 | Time: 4008.4s | Loss: 1665.8276
Train Acc: 0.9543 | Train Balanced Acc: 0.9105
Val   Acc: 0.8257 | Val   Balanced Acc: 0.4970

Epoch 093 | Time: 3910.3s | Loss: 1650.9134
Train Acc: 0.9545 | Train Balanced Acc: 0.9114
Val   Acc: 0.8243 | Val   Balanced Acc: 0.5082

Epoch 094 | Time: 3921.2s | Loss: 1630.4786
Train Acc: 0.9550 | Train Balanced Acc: 0.9112
Val   Acc: 0.8226 | Val   Balanced Acc: 0.4993

Epoch 095 | Time: 3941.2s | Loss: 1620.8651
Train Acc: 0.9553 | Train Balanced Acc: 0.9130
Val   Acc: 0.8237 | Val   Balanced Acc: 0.5113

Epoch 096 | Time: 3915.1s | Loss: 1608.0798
Train Acc: 0.9555 | Train Balanced Acc: 0.9128
Val   Acc: 0.8266 | Val   Balanced Acc: 0.4966

Epoch 097 | Time: 3908.0s | Loss: 1608.4898
Train Acc: 0.9556 | Train Balanced Acc: 0.9112
Val   Acc: 0.8197 | Val   Balanced Acc: 0.5015

Epoch 098 | Time: 3999.8s | Loss: 1581.2978
Train Acc: 0.9562 | Train Balanced Acc: 0.9155
Val   Acc: 0.8239 | Val   Balanced Acc: 0.5095

Epoch 099 | Time: 4027.9s | Loss: 1571.0071
Train Acc: 0.9564 | Train Balanced Acc: 0.9172
Val   Acc: 0.8232 | Val   Balanced Acc: 0.4997

Epoch 100 | Time: 4122.1s | Loss: 1564.7381
Train Acc: 0.9567 | Train Balanced Acc: 0.9158
Val   Acc: 0.8246 | Val   Balanced Acc: 0.4942
Checkpoint saved at: C:\Users\Boss\Downloads\malnet-images\checkpoints\epoch_100.pt
Model Accuracy: 82.40%
